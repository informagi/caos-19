{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "import xml.etree.ElementTree as Et\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from sklearn.model_selection import KFold\n",
    "#from sklearn.metrics import ndcg_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'pysearch' from 'pyserini.search' (C:\\Users\\Allemaal\\Anaconda3\\lib\\site-packages\\pyserini\\search\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b35dd3835261>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyserini\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpysearch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyserini\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyquerybuilder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyserini\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyserini\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalysis\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyanalysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyserini\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyclass\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mautoclass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'pysearch' from 'pyserini.search' (C:\\Users\\Allemaal\\Anaconda3\\lib\\site-packages\\pyserini\\search\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from pyserini.search import pysearch\n",
    "from pyserini.search import pyquerybuilder\n",
    "from pyserini.index import pyutils\n",
    "from pyserini.analysis import pyanalysis\n",
    "from pyserini.pyclass import autoclass\n",
    "from pyserini.analysis.pyanalysis import get_lucene_analyzer\n",
    "\n",
    "#Mirrors of old indices are archived here\n",
    "#https://github.com/castorini/anserini/blob/master/docs/experiments-cord19.md\n",
    "index_loc = '/home/tmschoegje/Desktop/caos-19/lucene-index-cord19-paragraph-2020-05-19/'\n",
    "searcher = pysearch.SimpleSearcher(index_loc)\n",
    "index_utils = pyutils.IndexReaderUtils(index_loc)\n",
    "\n",
    "#Additionally, you need the metadata.csv of the corresponding index, which is included in the CORD-19 releases\n",
    "#https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases.html\n",
    "metadatafile = \"/home/tmschoegje/Desktop/caos-19/metadata.csv\"\n",
    "\n",
    "\n",
    "docidfile = '/home/tmschoegje/Desktop/caos-19/trecdata/docids-rnd2.txt'\n",
    "topicsfile = \"/home/tmschoegje/Desktop/caos-19/trecdata/topics-rnd2.xml\"\n",
    "qrelname = \"/home/tmschoegje/Desktop/caos-19/trecdata/qrels-rnd2.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual topic classification into tasks\n",
    "rnd3classes = [2, 0, 3, 0, 3, 7, 7, 7, 6, 5, 4, 5, 0, 0, 0, 0, 4, 5, 5, 1, 0, 1, 1, 1, 1, 7, 7, 3, 3, 3, 2, 2, 3, 3, 9, 2, 2, 3, 3, 2]\n",
    "rnd3confidence = [1, 1, 1, 0.5, 0.5, 1, 1, 0.5, 0.5, 1, 0, 0.5, 1, 0, 1, 1, 0.75, 0.25, 0.25, 0.5, 0.5, 0.75, 0.75, 0.75, 0.75, 1, 0.5, 1, 1, 1, 0, 1, 1, 0, 0, 0.5, 1, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to read in metadata for the docids this round\n",
    "def prepTREC(fname):\n",
    "    #get valid TREC ids for this round\n",
    "    TRECids = []\n",
    "    f = open(fname)\n",
    "    for line in f.readlines():\n",
    "        if line[-1] == '\\n':\n",
    "            line = line[:-1]\n",
    "        TRECids.append(line)\n",
    "    f.close()\n",
    "    \n",
    "    metadata = pd.read_csv(metadatafile)\n",
    "    #now we filter all TREC ids we don't need\n",
    "    metadata = metadata[metadata.cord_uid.isin(TRECids)]\n",
    "    metadata.drop_duplicates(subset='cord_uid', keep='first', inplace=True)\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "#Used to read journal priors from the doc\n",
    "def prepJournals(fname):\n",
    "    f = open(fname)\n",
    "    journals = dict()\n",
    "    for line in f.readlines():\n",
    "        if line[-1] == '\\n':\n",
    "            line = line[:-1]\n",
    "        ls = line.split(\" \")\n",
    "        journals[ls[1]] = ls[0]\n",
    "    f.close()\n",
    "\n",
    "    return journals\n",
    "\n",
    "#Used to get a specific journal's prior value\n",
    "#cord_uid is id of document, metadata contains metadata.csv, \n",
    "#journals is a list of journals from journalpriors.txt (see prepJournals)\n",
    "def getJPrior(cord_uid, metadata, journals):\n",
    "    #Get journal for this item\n",
    "    journal = metadata[metadata['cord_uid'] == cord_uid]['journal']\n",
    "    if journal.to_string(index=False).strip() in journals:\n",
    "        return journals[journal.to_string(index=False).strip()]\n",
    "    else:\n",
    "        #if we have no knowledge, we assume the relevance is 0 (neutral)\n",
    "        return 0\n",
    "\n",
    "#Used to read TREC topics\n",
    "def readTopics(fname):\n",
    "    root = Et.parse(fname).getroot()\n",
    "    topics = []\n",
    "    for num, topic in enumerate(root):\n",
    "        #print(topic[0].text) #query\n",
    "        topics.append([topic[0].text, rnd3classes[num]])\n",
    "        #print(topic[1].text) #question\n",
    "        #print(topic[2].text) #narrative\n",
    "    return topics\n",
    "\n",
    "#Used to read in a run's ranking\n",
    "def readAnserini(fname):\n",
    "    res=[]\n",
    "    f = open(fname)\n",
    "    #f.readline()\n",
    "    prev_topic = 0\n",
    "    topic_count = 0\n",
    "    for line in f.readlines():\n",
    "        vals = line.strip().split(\" \")\n",
    "        if(vals[0] != prev_topic):\n",
    "            prev_topic = vals[0]\n",
    "            topic_count = 0\n",
    "\n",
    "        if(topic_count < 1000):\n",
    "            #topic, rank, cord_id, score\n",
    "            res.append([vals[0], vals[3], vals[2], vals[4]])\n",
    "    return res\n",
    "\n",
    "#Used to prepare results in submission format\n",
    "def writeBM25results(results, runtitle):\n",
    "    f = open(runtitle, \"w\")\n",
    "    #topic, rank, cord_id, score\n",
    "    for result in results:\n",
    "        #print(result)\n",
    "        f.write(result[0] + \" Q0 \" + result[2] + \" 1 \" + str(result[3]) + \" \" + runtitle + \"\\n\")\n",
    "    f.close()\n",
    "    \n",
    "\n",
    "#read the qrels file\n",
    "def getqrels(fname):\n",
    "    qrels = []\n",
    "    for line in open(fname).readlines():\n",
    "        vals = line.strip().split(\" \")\n",
    "        #topic, cord_uid, qrel, assessround\n",
    "        qrels.append([int(vals[0]), vals[3], float(vals[4]), float(vals[1])])\n",
    "\n",
    "    qrels = np.array(qrels, dtype=\"O\")\n",
    "    return qrels\n",
    "\n",
    "#find qrel for a cord uid\n",
    "def get_qrel(cord_uid, topic_id, qrels):\n",
    "\n",
    "    topicrels = qrels[qrels[:,0] == topic_id]\n",
    "\n",
    "    qrel_uids = [qrel[1] for qrel in topicrels]\n",
    "    #print(qrel_uids.index(cord_uid))\n",
    "    index = qrel_uids.index(cord_uid)\n",
    "    #print(qrels[index,2])\n",
    "    if(qrels[index,2] > 0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ndcg after filtering unknown docs - sakai 2007 says this is more stable than bpref\n",
    "\n",
    "#Note: it's nicer to do NDCG over all known qrels. \n",
    "#Implementation of this was limited - so we only considered the qrels in the top 30k documents\n",
    "\n",
    "def ndcg(runname, qrelname):\n",
    "    qrels = getqrels(qrelname)\n",
    "    \n",
    "    preds = []\n",
    "    #first parse predictions\n",
    "    for line in open(runname).readlines():\n",
    "        #topic, unused, cord_uid, rank, score, runname\n",
    "        vals = line.strip().split(\" \")\n",
    "        #topic, cord_uid, score, rank\n",
    "        preds.append([int(vals[0]), vals[2], float(vals[4]), int(vals[3])])\n",
    "        \n",
    "    #print('hi')\n",
    "    #print(len(preds))\n",
    "    #print(len(qrels))\n",
    "    knownpreds = []\n",
    "    \n",
    "    for num, pred in enumerate(preds):\n",
    "        #get qrels for the given topic\n",
    "        qrels_topic = qrels[qrels[:,0] == pred[0]]\n",
    "        qrel_uids = [val[1] for val in qrels_topic]\n",
    "\n",
    "        #filter all preds not in qrels\n",
    "        if(pred[1] in qrel_uids):\n",
    "            #add known prediction with predicted score and real score\n",
    "            knownpreds.append([pred[0], pred[1], pred[2], get_qrel(pred[1], pred[0], qrels_topic), pred[3]])\n",
    "            \n",
    "    knownpreds = np.array(knownpreds, dtype=\"O\")\n",
    "    \n",
    "    #TODO update for round2+ topics\n",
    "    ndcgs = []\n",
    "    for t in range(1, 31):\n",
    "        knownpreds_t = knownpreds[knownpreds[:,0] == t]\n",
    "        \n",
    "        #cross validation on knownpreds_t\n",
    "        #If this topic has at least 5 documens with known qrels, we compute it using 5x cross validation\n",
    "        #Otherwise, we ignore the ndcg for this topic. Afterwards, average for all topics\n",
    "        \n",
    "        n_splits = 5\n",
    "        if(len(knownpreds_t) > 5):\n",
    "            kf = KFold(n_splits)\n",
    "            for train_index, test_index in kf.split(knownpreds_t):\n",
    "        \n",
    "                sortedqrel = []\n",
    "                sortedqpred = []\n",
    "                for pred_ind, pred in enumerate(knownpreds_t):\n",
    "                    if pred_ind in train_index:\n",
    "                        #get corresponding pred's qrel\n",
    "                        sortedqrel.append(pred[3])#get_qrel(pred[1], pred[0], qrels))\n",
    "                        #ground truth\n",
    "                        sortedqpred.append(pred[2])        \n",
    "            \n",
    "                if(len(sortedqrel) > 1):\n",
    "                    ndcgs.append(ndcg_score(np.asarray([sortedqrel]), np.asarray([sortedqpred])))\n",
    "                else:\n",
    "                    print('how did i get here')\n",
    "           \n",
    "        else:\n",
    "            pass           \n",
    "        \n",
    "    return np.mean(ndcgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'readAnserini' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-75d4cf641848>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m#Currently testing on the baseline (using query terms)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreadAnserini\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/home/tmschoegje/Desktop/caos-19/baselines/r3.rf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mjournals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepJournals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/home/tmschoegje/Desktop/caos-19/round3/journalpriors.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepTREC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocidfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'readAnserini' is not defined"
     ]
    }
   ],
   "source": [
    "# Rerank by journal\n",
    "def rerank(res, topics, mixer, journals):\n",
    "    \n",
    "    for i in tqdm(range(0, len(res))):\n",
    "        jscore = getJPrior(res[i][2], metadata, journals)\n",
    "        score = float(res[i][3])\n",
    "        res[i][3] = mixer * float(jscore) + score\n",
    "            \n",
    "    #Some ugly/quick sorting\n",
    "    def sort_key0(item):\n",
    "        return item[3]\n",
    "    def sort_key1(item):\n",
    "        return item[0]\n",
    "\n",
    "    res = sorted(res, key=sort_key0, reverse=True)\n",
    "    res = sorted(res, key=sort_key1, reverse=False)\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "#Currently testing on the baseline (using query terms)\n",
    "results = readAnserini('/home/tmschoegje/Desktop/caos-19/baselines/r3.rf')\n",
    "journals = prepJournals('/home/tmschoegje/Desktop/caos-19/round3/journalpriors.txt')\n",
    "metadata = prepTREC(docidfile)\n",
    "print('loaded')\n",
    "\n",
    "# Let's see what linear combination between the run score these two values makes sense\n",
    "for m3 in (np.linspace(0, 10, 11)):\n",
    "    \n",
    "    #This is currently the best run. Differs from the submitted runfile because it is a longer list of ranked\n",
    "    #qrels that we can use to tune with ndcg\n",
    "    #results = readAnserini('/home/tmschoegje/Desktop/caos-19/runs/testrun-best-rnd3.run')\n",
    "    #results = readAnserini(\"/home/tmschoegje/Desktop/caos-19/trecdata/qexp-q0.4n0.4t0.2.run\")\n",
    "    \n",
    "    #print(len(results))\n",
    "    results_reranked = rerank(results, readTopics(topicsfile), m3, journals)\n",
    "    print('reranked')\n",
    "    writeBM25results(results_reranked, \"/home/tmschoegje/Desktop/caos-19/runs/jp-\" + str(m3) + '.run')\n",
    "#    print(compute_map(\"/home/tmschoegje/Desktop/caos-19/runs/testrun-\" + str(m3) + '.run'))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/tmschoegje/Desktop/caos-19/trecdata/docids-rnd3.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-8832502599e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mvalid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocid_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mvalid\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/tmschoegje/Desktop/caos-19/trecdata/docids-rnd3.txt'"
     ]
    }
   ],
   "source": [
    "#m = 0.25 gave 0.453\n",
    "\n",
    "\n",
    "docid_path = '/home/tmschoegje/Desktop/caos-19/trecdata/docids-rnd3.txt'\n",
    "train_path = '/home/tmschoegje/Desktop/caos-19/trecdata/qrels-rnd2.txt'\n",
    "test_path = '/home/tmschoegje/Desktop/caos-19/trecdata/qrels-covid_d3_j0.5-3.txt'\n",
    "topics_path = '/home/tmschoegje/Desktop/caos-19/trecdata/topics-rnd3.xml'\n",
    "train_n = 35\n",
    "test_n = 40\n",
    "\n",
    "valid = set()\n",
    "with open(docid_path, 'r') as f:\n",
    "    for line in f:\n",
    "        valid |= {line.strip()}\n",
    "\n",
    "tree = Et.parse(topics_path)\n",
    "root = tree.getroot()\n",
    "topics = [root[i][0].text for i in range(train_n)]\n",
    "nars =  [root[i][1].text for i in range(train_n)]\n",
    "\n",
    "train_ids = [[] for i in range(train_n)]\n",
    "train_vals = [[] for i in range(train_n)]\n",
    "with open(train_path, 'r') as f:\n",
    "    for line in f:\n",
    "        topicno, iteration, docid, relevance = line.strip().split(' ')\n",
    "        train_ids[int(topicno) - 1].append(docid)\n",
    "        train_vals[int(topicno) - 1].append(relevance)\n",
    "\n",
    "test_ids = [[] for i in range(test_n)]\n",
    "test_vals = [[] for i in range(test_n)]\n",
    "true_ids = [[] for i in range(test_n)]\n",
    "with open(test_path, 'r') as f:\n",
    "    for line in f:\n",
    "        topicno, iteration, docid, relevance = line.strip().split(' ')\n",
    "        test_ids[int(topicno) - 1].append(docid)\n",
    "        test_vals[int(topicno) - 1].append(relevance)\n",
    "        if(int(relevance) > 0):\n",
    "            true_ids[int(topicno) - 1].append(docid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20040865070199604\n",
      "0.0661914426251879\n",
      "0.12464951710452392\n",
      "0.16988628873571665\n",
      "0.19709898422847888\n"
     ]
    }
   ],
   "source": [
    "#true one\n",
    "def compute_map(run_name):\n",
    "    #used to limit how many preds we consider per topic (speeding up using the anserini baseline)\n",
    "    last_topic = 0\n",
    "    count_topic = 0\n",
    "    \n",
    "    #let's keep track of how many results are judged, and see if it corresponds with the leaderboards' j@ scores\n",
    "    judged = 0\n",
    "    counted = 0\n",
    "    \n",
    "    #average precision variables\n",
    "    #p@k\n",
    "    ap = 0\n",
    "    judged_topic = 0\n",
    "    gtp = 0\n",
    "    aps = []\n",
    "    allqrels = 0\n",
    "    vals = \"\"\n",
    "\n",
    "    for line in open(run_name, 'r').readlines():\n",
    "         \n",
    "        #topic, unused, cord_uid, rank, score, runname\n",
    "        vals = line.strip().split(\" \")\n",
    "        \n",
    "        #have we reached a new topic\n",
    "        if(last_topic != int(vals[0])):\n",
    "            #store AP from last topic\n",
    "            if(last_topic != 0):\n",
    "                #print(ap)\n",
    "                #aps.append((1.0 / qrels) * ap)\n",
    "                aps.append((1.0 / len(true_ids[int(vals[0]) - 1])) * ap)\n",
    "\n",
    "            last_topic = int(vals[0])\n",
    "            count_topic = 0            \n",
    "            qrels = 0\n",
    "            ap = 0\n",
    "            \n",
    "        #trec considers only the first 1000 predictions for each topic\n",
    "        if(count_topic < 1000):\n",
    "            count_topic += 1\n",
    "            qrel = 0\n",
    "\n",
    "            #if we know this value is true for this topic\n",
    "            if vals[2] in true_ids[int(vals[0])-1]:\n",
    "                qrel = 1\n",
    "                qrels += 1\n",
    "                allqrels +=1\n",
    "                \n",
    "            #add to the average precision\n",
    "            #print()\n",
    "            #print(qrels)\n",
    "            #print(count_topic)\n",
    "            ap += ((qrels / count_topic) * qrel)\n",
    "\n",
    "    #aps.append((1.0 / qrels) * ap)\n",
    "    aps.append((1.0 / len(true_ids[int(vals[0]) - 1])) * ap)\n",
    " \n",
    "    #print(aps)\n",
    "    return np.mean(aps)\n",
    "\n",
    "\n",
    "#for topic in true_ids:\n",
    "#    print(len(topic))\n",
    "    \n",
    "#print(compute_map(\"/home/tmschoegje/Desktop/caos-19/baselines/anserini.final-r3.rf.txt\"))\n",
    "print(compute_map(\"/home/tmschoegje/Desktop/caos-19/trecdata/r3.rf\"))\n",
    "print(compute_map(\"/home/tmschoegje/Desktop/caos-19/trecdata/ruir-round3.txt\"))\n",
    "print(compute_map(\"/home/tmschoegje/Desktop/caos-19/trecdata/qexp-q0.4n0.4t0.2.run\"))\n",
    "print(compute_map(\"/home/tmschoegje/Desktop/caos-19/baselines/r3.fusion1.txt\"))\n",
    "print(compute_map(\"/home/tmschoegje/Desktop/caos-19/baselines/r3.fusion2.txt\"))\n",
    "\n",
    "#TODO which set of true ids? shoudl we only count those in map\n",
    "\n",
    "#map for this run should be 0.2817"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
