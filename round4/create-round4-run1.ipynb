{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready\n"
     ]
    }
   ],
   "source": [
    "#note: one of the journaltests might be an easier starting point - they make explicit what files are necessary,\n",
    "#and this one was created/used more ad hoc\n",
    "print('ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'pysearch' from 'pyserini.search' (C:\\Users\\Allemaal\\Anaconda3\\lib\\site-packages\\pyserini\\search\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-be2b73a1c6db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyserini\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpysearch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyserini\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyquerybuilder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyserini\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyclass\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mautoclass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'pysearch' from 'pyserini.search' (C:\\Users\\Allemaal\\Anaconda3\\lib\\site-packages\\pyserini\\search\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as Et\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "\n",
    "import subprocess\n",
    "\n",
    "from pyserini.search import pysearch\n",
    "from pyserini.search import pyquerybuilder\n",
    "from pyserini.pyclass import autoclass\n",
    "from pyserini.analysis.pyanalysis import get_lucene_analyzer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'pyutils' from 'pyserini.index' (C:\\Users\\Allemaal\\Anaconda3\\lib\\site-packages\\pyserini\\index\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-22e770100493>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyserini\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyserini\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalysis\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyanalysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mindex_loc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'/home/tmschoegje/Desktop/caos-19/lucene-index-cord19-paragraph-2020-05-19/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msearcher\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpysearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSimpleSearcher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_loc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'pyutils' from 'pyserini.index' (C:\\Users\\Allemaal\\Anaconda3\\lib\\site-packages\\pyserini\\index\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from pyserini.index import pyutils\n",
    "from pyserini.analysis import pyanalysis\n",
    "\n",
    "index_loc = '/home/tmschoegje/Desktop/caos-19/lucene-index-cord19-paragraph-2020-05-19/'\n",
    "searcher = pysearch.SimpleSearcher(index_loc)\n",
    "index_utils = pyutils.IndexReaderUtils(index_loc)\n",
    "\n",
    "\n",
    "#mirrors of old indices are archived here\n",
    "#https://github.com/castorini/anserini/blob/master/docs/experiments-cord19.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docid_path = '/home/tmschoegje/Desktop/caos-19/trecdata/docids-rnd3.txt'\n",
    "valid = set()\n",
    "with open(docid_path, 'r') as f:\n",
    "    for line in f:\n",
    "        valid |= {line.strip()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_path = '/home/tmschoegje/Desktop/caos-19/trecdata/topics-rnd3.xml'\n",
    "qrel_path = '/home/tmschoegje/Desktop/caos-19/trecdata/qrels-covid_d3_j0.5-3.txt'#/home/tmschoegje/Desktop/caos-19/trecdata/qrels-covid_d3_j2.5-3.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = Et.parse(topics_path)\n",
    "root = tree.getroot()\n",
    "topics = [root[i][0].text for i in range(40)]\n",
    "nars =  [root[i][1].text for i in range(40)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "judged = defaultdict(set)\n",
    "with open(qrel_path, 'r') as f:\n",
    "    for line in f:\n",
    "        topicno, iteration, docid, relevance = line.strip().split(' ')\n",
    "        judged[int(topicno)] |= {docid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\n",
    "    ['environmental transmission', 'incubation', 'contagious', 'persistence', 'stability', 'physical', 'weather',\n",
    "     'epidemiology', 'shedding', 'reproductive number', 'modes of transmission', 'virulent', 'asymptomatic', 'pathogen',\n",
    "     'evolutionary host', 'transmission host'],\n",
    "    ['smoking', 'risk', 'pulmonary', 'pre-condition', 'co-infection', 'high-risk', 'severe', 'susceptible', 'fatality',\n",
    "     'neonates', 'respitory', 'condition', 'pre-existing', 'pregnant', 'morbidities'],\n",
    "    ['human-animal', 'origin', 'genetics', 'evolution', 'genome', 'sample sets', 'genomic', 'strain', 'livestock',\n",
    "     'animal host', 'natural history', 'genetic drift', 'mutation', 'genomics', 'sequencing'],\n",
    "    ['vaccine', 'therapeutic', 'treat', 'drugs', 'pharmaceuticals', 'recipients', 'ADE', 'complication', 'antiviral',\n",
    "     'prophylaxis', 'cloroquine', 'vaccination', 'immume respone'],\n",
    "    ['medical care', 'surge capacity', 'nursing home', 'allocation', 'personal protective equirement',\n",
    "     'clinical characterization', 'nursing', 'care', 'Extracorporeal membrane oxygenation', 'ECMO',\n",
    "     'mechanical ventilation', 'extrapulmonary manifestations', 'cardiomyopathy', 'cardiac arrest',\n",
    "     'regulatory standards', 'N95 masks', 'elastomeric respirators', 'telemedicine', 'steroids', 'high flow oxygen',\n",
    "     'supportive interventions'],\n",
    "    ['NPI', 'non-pharmaceutical intervention', 'school closure', 'travel ban', 'quarantine', 'mass gathering',\n",
    "     'social distancing', 'public health advice', 'economic impact'],\n",
    "    ['counties', 'geographic', 'geography', 'mortality rate', 'spread', 'mutations'],\n",
    "    ['diagnostic', 'surveillance', 'detection', 'screening', 'ELISAs', 'capacity', 'testing', 'point-of-care',\n",
    "     'rapid testing', 'pathogen', 'reagent', 'cytokines', 'response markers', 'swabs'],\n",
    "    ['ethical', 'social science', 'principles', 'standards', 'ethics', 'psychological health', 'fear', 'anxiety',\n",
    "     'stigma', 'sociology'],\n",
    "    ['collaboration', 'nomenclature', 'data standards', 'information sharing', 'communication', 'collaborate',\n",
    "     'coordination', 'misunderstanding', 'action plan']\n",
    "]\n",
    "\n",
    "topic_task_no = [2,0,3,0,3,6,3,6,7,5,4,5,0,0,0,0,3,5,5,1,0,0,1,1,6,6,6,3,3,3,2,2,3,1,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'common': 0.13407821229050282,\n",
       "  'important': 0.15083798882681568,\n",
       "  'known': 0.12849162011173187,\n",
       "  'mortality': 0.1675977653631285,\n",
       "  'one': 0.12290502793296092,\n",
       "  'transmission': 0.1843575418994414,\n",
       "  'vaccine': 0.11173184357541902},\n",
       " {'clinical': 0.14583333333333334,\n",
       "  'common': 0.05952380952380952,\n",
       "  'diagnosis': 0.05654761904761904,\n",
       "  'evidence': 0.08035714285714285,\n",
       "  'factors': 0.05952380952380952,\n",
       "  'high': 0.06845238095238095,\n",
       "  'highly': 0.05952380952380952,\n",
       "  'identified': 0.050595238095238096,\n",
       "  'known': 0.05952380952380952,\n",
       "  'large': 0.0625,\n",
       "  'risk': 0.08333333333333333,\n",
       "  'studies': 0.08035714285714285,\n",
       "  'system': 0.13392857142857142},\n",
       " {'animal': 0.0919881305637982,\n",
       "  'animals': 0.06231454005934717,\n",
       "  'early': 0.086053412462908,\n",
       "  'host': 0.10089020771513352,\n",
       "  'infected': 0.07121661721068248,\n",
       "  'large': 0.056379821958456956,\n",
       "  'mortality': 0.059347181008902065,\n",
       "  'reported': 0.08308605341246289,\n",
       "  'species': 0.0890207715133531,\n",
       "  'studies': 0.07121661721068248,\n",
       "  'three': 0.10385756676557863,\n",
       "  'two': 0.06528189910979226,\n",
       "  'vaccine': 0.059347181008902065},\n",
       " {'although': 0.10952380952380954,\n",
       "  'assay': 0.10000000000000002,\n",
       "  'development': 0.11428571428571431,\n",
       "  'effective': 0.10000000000000002,\n",
       "  'highly': 0.1285714285714286,\n",
       "  'protein': 0.13809523809523813,\n",
       "  'related': 0.09523809523809526,\n",
       "  'vaccines': 0.11428571428571431,\n",
       "  'years': 0.10000000000000002},\n",
       " {'care': 0.13068181818181818,\n",
       "  'clinical': 0.1590909090909091,\n",
       "  'confirmed': 0.051136363636363626,\n",
       "  'critical': 0.05397727272727272,\n",
       "  'data': 0.059659090909090905,\n",
       "  'diagnosis': 0.07954545454545454,\n",
       "  'evidence': 0.051136363636363626,\n",
       "  'identify': 0.051136363636363626,\n",
       "  'information': 0.051136363636363626,\n",
       "  'lower': 0.05397727272727272,\n",
       "  'may': 0.05397727272727272,\n",
       "  'provide': 0.07102272727272727,\n",
       "  'several': 0.07102272727272727,\n",
       "  'studies': 0.06249999999999999},\n",
       " {'although': 0.09633027522935783,\n",
       "  'based': 0.08715596330275231,\n",
       "  'care': 0.10091743119266057,\n",
       "  'control': 0.10550458715596332,\n",
       "  'evidence': 0.11467889908256884,\n",
       "  'important': 0.1284403669724771,\n",
       "  'large': 0.10091743119266057,\n",
       "  'model': 0.10550458715596332,\n",
       "  'non': 0.1605504587155964},\n",
       " {'care': 0.07942238267148015,\n",
       "  'control': 0.12635379061371843,\n",
       "  'developed': 0.07220216606498196,\n",
       "  'early': 0.11552346570397114,\n",
       "  'effective': 0.07220216606498196,\n",
       "  'genome': 0.07581227436823106,\n",
       "  'low': 0.07581227436823106,\n",
       "  'major': 0.07220216606498196,\n",
       "  'need': 0.1660649819494585,\n",
       "  'reported': 0.07220216606498196,\n",
       "  'years': 0.07220216606498196},\n",
       " {'clinical': 0.3448275862068965,\n",
       "  'common': 0.13103448275862067,\n",
       "  'detection': 0.11724137931034483,\n",
       "  'diagnosis': 0.1586206896551724,\n",
       "  'effective': 0.11724137931034483,\n",
       "  'major': 0.13103448275862067},\n",
       " {},\n",
       " {}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks_qrel = [{'transmission': 0.033, 'mortality': 0.03, 'important': 0.027, 'common': 0.024, 'known': 0.023, 'one': 0.022, 'vaccine': 0.02}, {'clinical': 0.049, 'system': 0.045, 'risk': 0.028, 'studies': 0.027, 'evidence': 0.027, 'high': 0.023, 'large': 0.021, 'known': 0.02, 'factors': 0.02, 'highly': 0.02, 'common': 0.02, 'diagnosis': 0.019, 'identified': 0.017}, {'three': 0.035, 'host': 0.034, 'animal': 0.031, 'species': 0.03, 'early': 0.029, 'reported': 0.028, 'studies': 0.024, 'infected': 0.024, 'two': 0.022, 'animals': 0.021, 'mortality': 0.02, 'vaccine': 0.02, 'large': 0.019}, {'protein': 0.029, 'highly': 0.027, 'development': 0.024, 'vaccines': 0.024, 'although': 0.023, 'years': 0.021, 'effective': 0.021, 'assay': 0.021, 'related': 0.02}, {'clinical': 0.056, 'care': 0.046, 'diagnosis': 0.028, 'several': 0.025, 'provide': 0.025, 'studies': 0.022, 'data': 0.021, 'may': 0.019, 'critical': 0.019, 'lower': 0.019, 'confirmed': 0.018, 'identify': 0.018, 'information': 0.018, 'evidence': 0.018}, {'non': 0.035, 'important': 0.028, 'evidence': 0.025, 'control': 0.023, 'model': 0.023, 'care': 0.022, 'large': 0.022, 'although': 0.021, 'based': 0.019}, {'need': 0.046, 'control': 0.035, 'early': 0.032, 'care': 0.022, 'genome': 0.021, 'low': 0.021, 'years': 0.02, 'reported': 0.02, 'effective': 0.02, 'major': 0.02, 'developed': 0.02}, {'clinical': 0.05, 'diagnosis': 0.023, 'common': 0.019, 'major': 0.019, 'detection': 0.017, 'effective': 0.017}, {}, {}]\n",
    "\n",
    "topic_task_no = [2,0,3,0,3,6,3,6,7,5,4,5,0,0,0,0,3,5,5,1,0,0,1,1,6,6,6,3,3,3,2,2,3,1,9]\n",
    "\n",
    "\n",
    "\n",
    "#classification with confidence\n",
    "rnd3classes = [2, 0, 3, 0, 3, 7, 7, 7, 6, 5, 4, 5, 0, 0, 0, 0, 4, 5, 5, 1, 0, 1, 1, 1, 1, 7, 7, 3, 3, 3, 2, 2, 3, 3, 9, 2, 2, 3, 3, 2]\n",
    "rnd3confidence = [1, 1, 1, 0.5, 0.5, 1, 1, 0.5, 0.5, 1, 0, 0.5, 1, 0, 1, 1, 0.75, 0.25, 0.25, 0.5, 0.5, 0.75, 0.75, 0.75, 0.75, 1, 0.5, 1, 1, 1, 0, 1, 1, 0, 0, 0.5, 1, 0, 0, 1]\n",
    "#create alias for chris' code.. should fix later\n",
    "topic_task_no = rnd3classes\n",
    "print(len(rnd3classes))\n",
    "\n",
    "tasks_weights = []\n",
    "for t in tasks_qrel:\n",
    "    total = sum([value for key, value in t.items()])\n",
    "    weights = dict()\n",
    "    sumweights = 0\n",
    "\n",
    "    for key, value in t.items():\n",
    "        weights[key] = value / total\n",
    "    tasks_weights.append(weights)\n",
    "tasks_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = {'of', 'to', 'the', 'for', 'in', 'on', 'and',\n",
    "         'is', 'will', 'Is', 'or', 'are', 'there', 'that', 'an', 'with',\n",
    "         'at', 'by', 'but', 'Are', 'be', 'this', 'if', 'they?'}\n",
    "\n",
    "def build_query(bm25, nars, task, bm25_split, nars_split, task_split, task_confidence, task_classes):\n",
    "    bm25 = [b for b in bm25 if b not in stops]\n",
    "    task = [t for t in task if t not in stops]\n",
    "    nars = [n for n in nars if n not in stops]\n",
    "    \n",
    "    builder = pyquerybuilder.get_boolean_query_builder()\n",
    "    \n",
    "    for b in bm25:\n",
    "        b = pyquerybuilder.get_term_query(b)\n",
    "        boostquery = pyquerybuilder.get_boost_query(b, bm25_split/len(bm25))\n",
    "        builder.add(boostquery, pyquerybuilder.JBooleanClauseOccur['should'].value)\n",
    "\n",
    "    #using terms from kaggle tasks\n",
    "    #if(task_confidence > 0.6):\n",
    "    for t in task:\n",
    "        t = pyquerybuilder.get_term_query(t)\n",
    "        boostquery = pyquerybuilder.get_boost_query(t, task_split/len(task) )#* task_confidence\n",
    "        builder.add(boostquery, pyquerybuilder.JBooleanClauseOccur['should'].value)\n",
    "    #print(task)\n",
    "    #using terms from qrel docs\n",
    "    #if(task_confidence > 0.8):\n",
    "    #    for t, boost in task.items():\n",
    "    #        t = pyquerybuilder.get_term_query(t)\n",
    "    #        boostquery = pyquerybuilder.get_boost_query(t, task_split/boost)\n",
    "    #        builder.add(boostquery, pyquerybuilder.JBooleanClauseOccur['should'].value)\n",
    "        \n",
    "    for n in nars:\n",
    "        try:\n",
    "            n = pyquerybuilder.get_term_query(n)\n",
    "        except:\n",
    "            print(n)\n",
    "            continue\n",
    "        boostquery = pyquerybuilder.get_boost_query(n, nars_split/len(nars))\n",
    "        builder.add(boostquery, pyquerybuilder.JBooleanClauseOccur['should'].value)\n",
    "        \n",
    "    return builder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "import numpy as np\n",
    "\n",
    "#load the qrels file\n",
    "def getqrels(fname):\n",
    "    qrels = []\n",
    "    #print(open(fname).readlines())\n",
    "    for line in open(fname, 'r').readlines():\n",
    "        vals = line.strip().split(\" \")\n",
    "        #topic, cord_uid, qrel, assessround\n",
    "        qrels.append([int(vals[0]), vals[2], float(vals[3]), float(vals[1])])\n",
    "\n",
    "    qrels = np.array(qrels, dtype=\"O\")\n",
    "    return qrels\n",
    "\n",
    "#find qrel for a cord uid\n",
    "def get_qrel(cord_uid, topic_id, qrels):\n",
    "\n",
    "    topicrels = qrels[qrels[:,0] == topic_id]\n",
    "\n",
    "    qrel_uids = [qrel[1] for qrel in topicrels]\n",
    "    #print(qrel_uids.index(cord_uid))\n",
    "    index = qrel_uids.index(cord_uid)\n",
    "    #print(qrels[index,2])\n",
    "    \n",
    "    \n",
    "    return qrels[index,2]\n",
    "#    if(qrels[index,2] > 0):\n",
    "#        return 1\n",
    "#    else:\n",
    "#        return 0\n",
    "\n",
    "\n",
    "#create a list of length N with K ones and N-K zeroes\n",
    "#was used for old version of kfold xvalidation\n",
    "def rand_bin_list(K, N):\n",
    "    arr = np.zeros(N)\n",
    "    arr[:K]  = 1\n",
    "    np.random.shuffle(arr)\n",
    "    return list(arr)\n",
    "\n",
    "\n",
    "def compute_map(runname, qrelname):\n",
    "    qrels = getqrels(qrelname)\n",
    "    preds = []\n",
    "    for line in open(runname, 'r').readlines():\n",
    "        #topic, unused, cord_uid, rank, score, runname\n",
    "        vals = line.strip().split(\" \")\n",
    "        #topic, cord_uid, score, rank\n",
    "        preds.append([int(vals[0]), vals[2], float(vals[4]), int(vals[3])])\n",
    "            \n",
    "    for num, pred in enumerate(preds):\n",
    "        #get qrels for the given topic\n",
    "        qrels_topic = qrels[qrels[:,0] == pred[0]]\n",
    "        qrel_uids = [val[1] for val in qrels_topic]\n",
    "    \n",
    "        #filter all preds not in qrels\n",
    "        #if(pred[1] in qrel_uids):\n",
    "        #add real val\n",
    "        \n",
    "        preds[num] = [pred[0], pred[1], pred[2], get_qrel(pred[1], pred[0], qrels_topic), pred[3]]\n",
    "        \n",
    "    preds = np.array(preds, dtype=\"O\")\n",
    "        \n",
    "    #assume we know all (because of how we generated)\n",
    "    aps = []\n",
    "    countval1 = 0\n",
    "    countval2 = 0\n",
    "    for t in range(1, 41):\n",
    "#        print(t)\n",
    "#        print(preds[:,0] == t)\n",
    "#        print(preds[0:5])\n",
    "        preds_t = preds[preds[:,0] == t]\n",
    "        \n",
    "        #cross validation on knownpreds_t\n",
    "        n_splits = 5\n",
    "        if(len(preds_t) > 5):\n",
    "            countval1 += 1\n",
    "            \n",
    "            kf = KFold(n_splits)\n",
    "            for train_index, test_index in kf.split(preds_t):\n",
    "                sortedqrel = []\n",
    "                sortedqpred = []\n",
    "                for pred_ind, pred in enumerate(preds_t):\n",
    "                    if pred_ind in train_index:\n",
    "                        #sortedqrel.append(pred[3])\n",
    "                        \n",
    "                        #make binary for map\n",
    "                        if(pred[3] > 0):\n",
    "                            sortedqrel.append(1)\n",
    "                        else:\n",
    "                            sortedqrel.append(0)\n",
    "                        sortedqpred.append(pred[2])\n",
    "            \n",
    "                \n",
    "                if(len(sortedqrel) > 1):\n",
    "#                    print(sortedqrel)\n",
    "                    aps.append(average_precision_score(np.asarray(sortedqrel), np.asarray(sortedqpred)))\n",
    "                else:\n",
    "                    print('huh')\n",
    "           \n",
    "        #copypasted from above.. should fix later\n",
    "        else:\n",
    "            countval2 += 1\n",
    "    print('How many times did we have enough qrels / didnt have')\n",
    "    print(countval1)\n",
    "    print(countval2)\n",
    "#    print(aps)\n",
    "    return np.mean(aps)\n",
    "\n",
    "#ndcg after filtering unknown docs - sakai 2007 says this is more stable than bpref\n",
    "def ndcg(runname, qrelname):\n",
    "    qrels = getqrels(qrelname)\n",
    "#    print(np.array(qrels).shape)\n",
    "    \n",
    "    preds = []\n",
    "    for line in open(runname, 'r').readlines():\n",
    "        #topic, unused, cord_uid, rank, score, runname\n",
    "        vals = line.strip().split(\" \")\n",
    "        #topic, cord_uid, score, rank\n",
    "        preds.append([int(vals[0]), vals[2], float(vals[4]), int(vals[3])])\n",
    "        \n",
    "#    print(np.array(preds).shape)\n",
    "\n",
    "    knownpreds = []\n",
    "    \n",
    "    for num, pred in enumerate(preds):\n",
    "        #get qrels for the given topic\n",
    "        qrels_topic = qrels[qrels[:,0] == pred[0]]\n",
    "        qrel_uids = [val[1] for val in qrels_topic]\n",
    "    \n",
    "        #filter all preds not in qrels\n",
    "        if(pred[1] in qrel_uids):\n",
    "            #add real val\n",
    "            knownpreds.append([pred[0], pred[1], pred[2], get_qrel(pred[1], pred[0], qrels_topic), pred[3]])\n",
    "            #print(str(pred[0]) + ' '+ str(pred[1]))\n",
    "\n",
    "    #prep data so we have\n",
    "    #true score, pred score\n",
    "    #print('so how many do we know')\n",
    "    #print(len(knownpreds))\n",
    "    #print(len(preds))\n",
    "    #print(knownpreds[0:5])\n",
    "    #print(preds[0:5])\n",
    "\n",
    "    knownpreds = np.array(knownpreds, dtype=\"O\")\n",
    "    #print(knownpreds.shape)\n",
    "\n",
    "        \n",
    "    #print(sortedqrel[0:20])\n",
    "    #print(sortedqpred[0:20])\n",
    "    \n",
    "    #TODO for each topic compute ndcg, then average\n",
    "    ndcgs = []\n",
    "    countval1 = 0\n",
    "    countval2 = 0\n",
    "    for t in range(1, 41):\n",
    "        #print('known preds')\n",
    "        #print(knownpreds)\n",
    "        #print(knownpreds[:,0])\n",
    "        #get this topic's results after filtering unknown qrels\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print('here')\n",
    "        #print(t)\n",
    "        #print(np.array(knownpreds).shape)\n",
    "        #print(knownpreds)\n",
    "        knownpreds_t = knownpreds[knownpreds[:,0] == t]\n",
    "        #anchor2\n",
    "\n",
    "        \n",
    "        #cross validation on knownpreds_t\n",
    "        n_splits = 5\n",
    "        if(len(knownpreds_t) > 5):\n",
    "            countval1 += 1\n",
    "    #        n_splits = 1\n",
    "            kf = KFold(n_splits)\n",
    "            for train_index, test_index in kf.split(knownpreds_t):\n",
    "            #print(train_index)\n",
    "#        for i in range(0, 5):\n",
    "            #if 1, ignore from this run\n",
    "#            crossval_ind = rand_bin_list(np.floor(len(knownpreds_ts) / 5), len(knownpreds_ts))\n",
    "#            print(crossval_ind)\n",
    "            \n",
    "\n",
    "        \n",
    "                sortedqrel = []\n",
    "                sortedqpred = []\n",
    "                for pred_ind, pred in enumerate(knownpreds_t):\n",
    "                    if pred_ind in train_index:\n",
    "                #if crossval_ind[pred_ind] == 0:\n",
    "                    #print('rankingnew task')\n",
    "                    #print(pred[1])\n",
    "                    #print(pred[0])\n",
    "                    #print(get_qrel(pred[1], pred[0], qrels))\n",
    "                    #get corresponding pred's qrel\n",
    "                        sortedqrel.append(pred[3])#get_qrel(pred[1], pred[0], qrels))\n",
    "                    #ground truth: is supposed to be a '2'\n",
    "                        sortedqpred.append(pred[2])\n",
    "            \n",
    "                #print('topic ' + str(t) + ' has qrels ' + str(len(sortedqrel)))#' knownpreds')\n",
    "                #print(knownpreds_t[0:5])\n",
    "                #print(sortedqrel[0:5])\n",
    "                #print(sortedqpred[0:5])\n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "                if(len(sortedqrel) > 1):\n",
    "                        #print(np.array([sortedqrel]))\n",
    "                        #print(np.array([sortedqpred]))\n",
    "        #            print(ndcg_score(np.asarray([sortedqrel]), np.asarray([sortedqpred]), k=5))\n",
    "                    ndcgs.append(ndcg_score(np.asarray([sortedqrel]), np.asarray([sortedqpred])))\n",
    "                else:\n",
    "                    pass\n",
    "                    #print('huh')\n",
    "           \n",
    "        #copypasted from above.. should fix later\n",
    "        else:\n",
    "            countval2 += 1\n",
    "#            sortedqrel = []\n",
    "#            sortedqpred = []\n",
    "#            for pred in knownpreds_t:\n",
    "#                sortedqrel.append(pred[3])\n",
    "#                sortedqpred.append(pred[2])\n",
    "            \n",
    "#                if(len(sortedqrel) > 1):\n",
    "#                        ndcgs.append(ndcg_score(np.asarray([sortedqrel]), np.asarray([sortedqpred]), k=10))\n",
    "#                else:\n",
    "#                    pass\n",
    "                    #print('huh')\n",
    "           \n",
    "        \n",
    "    print(countval1)\n",
    "    print(countval2)\n",
    "    #print(ndcgs)\n",
    "    return np.mean(ndcgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testrun2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e503d9af479e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrunname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/tmschoegje/Desktop/caos-19/runs/testrun-best-rnd4.run'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtestrun2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndcg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqrelname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#runname = \"/home/tmschoegje/Desktop/caos-19/doc2vectest3\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#qrelname = '/home/tmschoegje/Desktop/caos-19/trecdata/qrels-rnd2.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'testrun2' is not defined"
     ]
    }
   ],
   "source": [
    "runname = '/home/tmschoegje/Desktop/caos-19/runs/testrun-best-rnd4.run'\n",
    "testrun2(runname, 0.4, 0.4, 0.2)\n",
    "print(ndcg(runname, qrelname))\n",
    "#runname = \"/home/tmschoegje/Desktop/caos-19/doc2vectest3\"\n",
    "#qrelname = '/home/tmschoegje/Desktop/caos-19/trecdata/qrels-rnd2.txt'\n",
    "#print(ndcg(runname, qrelname))\n",
    "#print(ndcg('/home/tmschoegje/Desktop/caos-19/trecdata/testtasktext3', qrelname))\n",
    "\n",
    "#anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qrelname' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-55ca802e8e62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndcg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqrelname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'qrelname' is not defined"
     ]
    }
   ],
   "source": [
    "print(ndcg(runname, qrelname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from constants.py\n",
    "#has the contstants we use for doc2vec reranking\n",
    "\n",
    "#these were defined before\n",
    "#rnd1classes = [2,0,3,0,3,6,3,6,7,5,4,5,0,0,0,0,6,5,5,1,0,0,1,1,1,6,6,3,3,3]\n",
    "#rnd3classes = [2,0,3,0,3,7,7,7,6,5,4,5,0,0,0,0,4,5,5,1,0,1,1,1,1,7,7,3,3,3,, 2, 3, 3, 9, 2, 2, 3, 3, 2]\n",
    "#rnd2classes = [2,0,3,0,3,6,3,6,7,5,4,5,0,0,0,0,3,5,5,1,0,0,1,1,6,6,6,3,3,3,2,2,3,1,9]\n",
    "#rnd3confidence = [1, 1, 1, 0.5, 0.5, 1, 1, 0.5, 0.5, 1, 0, 0.5, 1, 0, 1, 1, 0.75, 0.25, 0.25, 0.5, 0.5, 0.75, 0.75, 0.75, 0.75, 1, 0.5, 1, 1, 1, 0, 1, 1, 0, 0, 0.5, 1, 0, 0, 1]\n",
    "\n",
    "task_1_short = \"transmission incubation environment\"\n",
    "task_2_short = \"risk factors\"\n",
    "task_3_short = \"genetics origin evolution\"\n",
    "task_4_short = \"vaccines therapeutics\"\n",
    "task_5_short = \"medical care\"\n",
    "task_6_short = \"effectiveness non pharmaceutical interventions\"\n",
    "task_7_short = \"diagnostics surveillance\"\n",
    "task_8_short = \"geographic spread\"\n",
    "task_9_short = \"ethical social considerations\"\n",
    "task_10_short = \"information sharing collaboration\"\n",
    "\n",
    "task_1 = \"What is known about transmission, incubation, and environmental stability? What do we know about natural history, transmission, and diagnostics for the virus? What have we learned about infection prevention and control? Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery. Prevalence of asymptomatic shedding and transmission (e.g., particularly children). Seasonality of transmission. Physical science of the coronavirus (e.g., charge distribution, adhesion to hydrophilic/phobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding). Persistence and stability on a multitude of substrates and sources (e.g., nasal discharge, sputum, urine, fecal matter, blood). Persistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic). Natural history of the virus and shedding of it from an infected person. Implementation of diagnostics and products to improve clinical processes. Disease models, including animal models for infection, disease and transmission. Tools and studies to monitor phenotypic change and potential adaptation of the virus. Immune response and immunity. Effectiveness of movement control strategies to prevent secondary transmission in health care and community settings. Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings. Role of the environment in transmission\"\n",
    "task_2 = \"What do we know about COVID-19 risk factors? What have we learned from epidemiological studies? Data on potential risks factors. Smoking, pre-existing pulmonary disease. Co-infections (determine whether co-existing respiratory/viral infections make the virus more transmissible or virulent) and other co-morbidities. Neonates and pregnant women. Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences. Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors. Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups. Susceptibility of populations. Public health mitigation measures that could be effective for control.\"\n",
    "task_3 = \"What do we know about virus genetics, origin, and evolution? What do we know about the virus origin and management measures at the human-animal interface? Real-time tracking of whole genomes and a mechanism for coordinating the rapid dissemination of that information to inform the development of diagnostics and therapeutics and to track variations of the virus over time. Access to geographic and temporal diverse sample sets to understand geographic distribution and genomic differences, and determine whether there is more than one strain in circulation. Multi-lateral agreements such as the Nagoya Protocol could be leveraged. Evidence that livestock could be infected (e.g., field surveillance, genetic sequencing, receptor binding) and serve as a reservoir after the epidemic appears to be over. Evidence of whether farmers are infected, and whether farmers could have played a role in the origin. Surveillance of mixed wildlife- livestock farms for SARS-CoV-2 and other coronaviruses in Southeast Asia. Experimental infections to test host range for this pathogen. Animal host(s) and any evidence of continued spill-over to humans. Socioeconomic and behavioral risk factors for this spill-over. Sustainable risk reduction strategies\"\n",
    "task_4 = \"What do we know about vaccines and therapeutics? What has been published concerning research and development and evaluation efforts of vaccines and therapeutics? Effectiveness of drugs being developed and tried to treat COVID-19 patients. Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocycline that that may exert effects on viral replication. Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients. Exploration of use of best animal models and their predictive value for a human vaccine. Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents. Alternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up. This could include identifying approaches for expanding production capacity to ensure equitable and timely distribution to populations in need. Efforts targeted at a universal coronavirus vaccine. Efforts to develop animal models and standardize challenge studies. Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers. Approaches to evaluate risk for enhanced disease after vaccination. Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models in conjunction with therapeutics.\"\n",
    "task_5 = \"What has been published about medical care? What has been published concerning surge capacity and nursing homes? What has been published concerning efforts to inform allocation of scarce resources? What do we know about personal protective equipment? What has been published concerning alternative methods to advise on disease management? What has been published concerning processes of care? What do we know about the clinical characterization and management of the virus? Resources to support skilled nursing facilities and long term care facilities. Mobilization of surge medical staff to address shortages in overwhelmed communities. Age-adjusted mortality data for Acute Respiratory Distress Syndrome (ARDS) with/without other organ failure – particularly for viral etiologies. Extracorporeal membrane oxygenation (ECMO) outcomes data of COVID-19 patients. Outcomes data for COVID-19 after mechanical ventilation adjusted for age. Application of regulatory standards (e.g., EUA, CLIA) and ability to adapt care to crisis standards of care level. Approaches for encouraging and facilitating the production of elastomeric respirators, which can save thousands of N95 masks. Best telemedicine practices, barriers and facilitators, and specific actions to remove/expand them within and across state boundaries. Guidance on the simple things people can do at home to take care of sick people and manage disease. Oral medications that might potentially work. Use of AI in real-time health care delivery to evaluate interventions, risk factors, and outcomes in a way that could not be done manually. Best practices and critical challenges and innovative solutions and technologies in hospital flow and organization, workforce protection, workforce allocation, community-based support resources, payment, and supply chain management to enhance capacity, efficiency, and outcomes. Efforts to define the natural history of disease to inform clinical care, public health interventions, infection prevention control, transmission, and clinical trials. Efforts to develop a core clinical outcome set to maximize usability of data across a range of trials. Efforts to determine adjunctive and supportive interventions that can improve the clinical outcomes of infected patients (e.g. steroids, high flow oxygen). \"\n",
    "task_6 = \"What do we know about the effectiveness of non-pharmaceutical interventions? What is known about equity and barriers to compliance for non-pharmaceutical interventions? Guidance on ways to scale up NPIs in a more coordinated way (e.g., establish funding, infrastructure and authorities to support real time, authoritative (qualified participants) collaboration with all states to gain consensus on consistent guidance and to mobilize resources to geographic areas where critical shortfalls are identified) to give us time to enhance our health care delivery system capacity to respond to an increase in cases. Rapid design and execution of experiments to examine and compare NPIs currently being implemented. DHS Centers for Excellence could potentially be leveraged to conduct these experiments. Rapid assessment of the likely efficacy of school closures, travel bans, bans on mass gatherings of various sizes, and other social distancing approaches. Methods to control the spread in communities, barriers to compliance and how these vary among different populations. Models of potential interventions to predict costs and benefits that take account of such factors as race, income, disability, age, geographic location, immigration status, housing status, employment status, and health insurance status. Policy changes necessary to enable the compliance of individuals with limited resources and the underserved with NPIs. Research on why people fail to comply with public health advice, even if they want to do so (e.g., social or financial costs may be too high). Research on the economic impact of this or any pandemic. This would include identifying policy and programmatic alternatives that lessen/mitigate risks to critical government services, food distribution and supplies, access to critical household supplies, and access to health diagnoses, treatment, and needed care, regardless of ability to pay.\"\n",
    "task_7 = \"What do we know about diagnostics and surveillance? What has been published concerning systematic, holistic approach to diagnostics (from the public health surveillance perspective to being able to predict clinical outcomes)? How widespread current exposure is to be able to make immediate policy recommendations on mitigation measures. Denominators for testing and a mechanism for rapidly sharing that information, including demographics, to the extent possible. Sampling methods to determine asymptomatic disease (e.g., use of serosurveys (such as convalescent samples) and early detection of disease (e.g., use of screening of neutralizing antibodies such as ELISAs). Efforts to increase capacity on existing diagnostic platforms and tap into existing surveillance platforms. Recruitment, support, and coordination of local expertise and capacity (public, private—commercial, and non-profit, including academic), including legal, ethical, communications, and operational issues. National guidance and guidelines about best practices to states (e.g., how states might leverage universities and private laboratories for testing purposes, communications to public health officials and the public). Development of a point-of-care test (like a rapid influenza test) and rapid bed-side tests, recognizing the tradeoffs between speed, accessibility, and accuracy. Rapid design and execution of targeted surveillance experiments calling for all potential testers using PCR in a defined area to start testing and report to a specific entity. These experiments could aid in collecting longitudinal samples, which are critical to understanding the impact of ad hoc local interventions (which also need to be recorded). Separation of assay development issues from instruments, and the role of the private sector to help quickly migrate assays onto those devices. Efforts to track the evolution of the virus (i.e., genetic drift or mutations) and avoid locking into specific reagents and surveillance/detection schemes. Latency issues and when there is sufficient viral load to detect the pathogen, and understanding of what is needed in terms of biological and environmental sampling. Use of diagnostics such as host response markers (e.g., cytokines) to detect early disease or predict severe disease progression, which would be important to understanding best clinical practice and efficacy of therapeutic interventions. Policies and protocols for screening and testing. Policies to mitigate the effects on supplies associated with mass testing, including swabs and reagents. Technology roadmap for diagnostics. Barriers to developing and scaling up new diagnostic tests (e.g., market forces), how future coalition and accelerator models (e.g., Coalition for Epidemic Preparedness Innovations) could provide critical funding for diagnostics, and opportunities for a streamlined regulatory environment. New platforms and technology (e.g., CRISPR) to improve response times and employ more holistic approaches to COVID-19 and future diseases. Coupling genomics and diagnostic testing on a large scale. Enhance capabilities for rapid sequencing and bioinformatics to target regions of the genome that will allow specificity for a particular variant. Enhance capacity (people, technology, data) for sequencing with advanced analytics for unknown pathogens, and explore capabilities for distinguishing naturally-occurring pathogens from intentional. One Health surveillance of humans and potential sources of future spillover or ongoing exposure for this organism and future pathogens, including both evolutionary hosts (e.g., bats) and transmission hosts (e.g., heavily trafficked and farmed wildlife and domestic food and companion species), inclusive of environmental, demographic, and occupational risk factors.\"\n",
    "task_8 = \"At the time of writing, COVID-19 has spread to at least 114 countries. With viral flu, there are often geographic variations in how the disease will spread and if there are different variations of the virus in different areas. We’d like to explore what the literature and data say about this through this Task. Are there geographic variations in the rate of COVID-19 spread? Are there geographic variations in the mortality rate of COVID-19? Is there any evidence to suggest geographic based virus mutations?\"\n",
    "task_9 = \"What has been published concerning ethical considerations for research? What has been published concerning social sciences at the outbreak response? Efforts to articulate and translate existing ethical principles and standards to salient issues in COVID-2019. Efforts to embed ethics across all thematic areas, engage with novel ethical issues that arise and coordinate to minimize duplication of oversight. Efforts to support sustained education, access, and capacity building in the area of ethics. Efforts to establish a team at WHO that will be integrated within multidisciplinary research and operational platforms and that will connect with existing and expanded global networks of social sciences. Efforts to develop qualitative assessment frameworks to systematically collect information related to local barriers and enablers for the uptake and adherence to public health measures for prevention and control. This includes the rapid identification of the secondary impacts of these measures. (e.g. use of surgical masks, modification of health seeking behaviors for SRH, school closures). Efforts to identify how the burden of responding to the outbreak and implementing public health measures affects the physical and psychological health of those providing care for Covid-19 patients and identify the immediate needs that must be addressed. Efforts to identify the underlying drivers of fear, anxiety and stigma that fuel misinformation and rumor, particularly through social media.\"\n",
    "task_10 = \"What has been published about information sharing and inter-sectoral collaboration? What has been published about data standards and nomenclature? What has been published about governmental public health? What do we know about risk communication? What has been published about communicating with high-risk populations? What has been published to clarify community measures? What has been published about equity considerations and problems of inequity? Methods for coordinating data-gathering with standardized nomenclature. Sharing response information among planners, providers, and others. Understanding and mitigating barriers to information-sharing. How to recruit, support, and coordinate local (non-Federal) expertise and capacity relevant to public health emergency response (public, private, commercial and non-profit, including academic). Integration of federal/state/local public health surveillance systems. Value of investments in baseline public health response infrastructure preparedness. Modes of communicating with target high-risk populations (elderly, health care workers). Risk communication and guidelines that are easy to understand and follow (include targeting at risk populations’ families too). Communication that indicates potential risk of disease to all population groups. Misunderstanding around containment and mitigation. Action plan to mitigate gaps and problems of inequity in the Nation’s public health capability, capacity, and funding to ensure all citizens in need are supported and can access information, surveillance, and treatment. Measures to reach marginalized and disadvantaged populations. Data systems and research priorities and agendas incorporate attention to the needs and circumstances of disadvantaged populations and underrepresented minorities. Mitigating threats to incarcerated people from COVID-19, assuring access to information, prevention, diagnosis, and treatment. Understanding coverage policies (barriers and opportunities) related to testing, treatment, and care.\"\n",
    "\n",
    "\n",
    "list_of_tasks = [task_1, task_2, task_3, task_4, task_5, task_6, task_7, task_8, task_9, task_10]\n",
    "list_of_tasks_short = [task_1_short, task_2_short, task_3_short, task_4_short, task_5_short, task_6_short, task_7_short, task_8_short, task_9_short, task_10_short]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from prep.py\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd \n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "def readTopics(fname):\n",
    "    root = Et.parse(fname).getroot()\n",
    "    topics = []\n",
    "    for num, topic in enumerate(root):\n",
    "        #print(topic[0].text) #query\n",
    "        topics.append([topic[0].text, rnd3classes[num]])\n",
    "        #print(topic[1].text) #question\n",
    "        #print(topic[2].text) #narrative\n",
    "    return topics\n",
    "\n",
    "def preparedoc2vec(fname, data):\n",
    "    #Check if a model exists\n",
    "    if(os.path.isfile(fname)):\n",
    "        print(\"Loaded doc2vec model \" + fname)\n",
    "        model = Doc2Vec.load(fname)\n",
    "    else:\n",
    "        print(\"Training doc2vec model \" + fname)\n",
    "        \n",
    "        #Remove items with bad abstracts\n",
    "        docs = data[~data.abstract.isin([\"Unknown\", \"unknown\", \"\"])]\n",
    "        #remove items with mising abstracts\n",
    "        docs = data[~data.abstract.isnull()]\n",
    "        metadatana = docs\n",
    "        docvals = docs['abstract']\n",
    "        docvals = docvals.values.tolist()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        documents = [TaggedDocument(gensim.parsing.preprocess_string(doc), [i]) for i, doc in enumerate(docvals)]\n",
    "        #print('NUMBER OF DOCS ' + str(np.array(documents)))\n",
    "        \n",
    "        #this used to be trained on the processedfiles, but abstract is available in metadata in new version\n",
    "        #[TaggedDocument(doc[4], [i]) for i, doc in enumerate(cleaned_files)]\n",
    "        #print(\"Sanity check: is this an abstract?\")\n",
    "        #print(documents[0])\n",
    "        model = gensim.models.doc2vec.Doc2Vec(dm=1, vector_size=100, min_count=2, epochs=20, seed=42, workers=3)\n",
    "        model.build_vocab(documents)\n",
    "        model.train(documents, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "        \n",
    "        model.save(fname)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def prepTREC(fname):\n",
    "    #get valid TREC ids for this round\n",
    "    TRECids = []\n",
    "    f = open(fname, 'r')\n",
    "    for line in f.readlines():\n",
    "        if line[-1] == '\\n':\n",
    "            line = line[:-1]\n",
    "        TRECids.append(line)\n",
    "    f.close()\n",
    "    \n",
    "    metadata = pd.read_csv(\"/home/tmschoegje/Desktop/caos-19/metadata.csv\")\n",
    "    #now we filter all TREC ids we don't need\n",
    "    #print(np.array(metadata).shape)\n",
    "    #print(np.array(TRECids).shape)\n",
    "    metadata = metadata[metadata.cord_uid.isin(TRECids)]\n",
    "    #there's 33 duplicates (same paper from multiple sources)\n",
    "    metadata.drop_duplicates(subset='cord_uid', keep='first', inplace=True)\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "#Used to store/load scores of docs to the tasks\n",
    "def docToTaskScores(fname):\n",
    "    scores = []\n",
    "    if(os.path.isfile(fname)):\n",
    "        print(\"Loaded document-to-task scores from \" + fname)\n",
    "        for line in open(fname, 'r'):\n",
    "            scores.append(line.replace(\"\\n\", \"\").split(\" \"))\n",
    "#        print(scores)\n",
    "    else:\n",
    "        print(\"Storing document-to-task scores in \" + fname)\n",
    "        f = open(fname, \"w\")        \n",
    "        \n",
    "        for file in cleaned_files:\n",
    "            newline = str(file[0])\n",
    "            for index, task in enumerate(list_of_tasks):\n",
    "                #If there is no abstract, we say it's very far away for now.\n",
    "                #Based on assumption that these are less valuable\n",
    "                #Alternative: use the last 200 words\n",
    "                #print(\" \".join(file[5].split(\" \")[-200:]))\n",
    "                dist = np.linalg.norm(taskvectors[index]-get_doc_vector(model, \" \".join(file[5].split(\" \")[-200:])))\n",
    "                newline += \" \" + str(dist)\n",
    "                \n",
    "            f.write(newline + \"\\n\")\n",
    "            scores.append(newline.split(\" \"))\n",
    "                \n",
    "        f.close()\n",
    "    return scores\n",
    "\n",
    "def get_doc_vector(model, doc):\n",
    "    tokens = gensim.parsing.preprocess_string(doc)\n",
    "    vector = model.infer_vector(tokens)\n",
    "    return vector\n",
    "\n",
    "def getAbstract(cord_id, metadata):\n",
    "    abstract = metadata[metadata['cord_uid'] == cord_id]['abstract']\n",
    "    return abstract.to_string()\n",
    "\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "def getJPrior(cord_uid, metadata, journals):\n",
    "    journal = metadata[metadata['cord_uid'] == cord_uid]['journal']\n",
    "    #print(journals)\n",
    "    #print(journal.to_string())\n",
    "    if journal.to_string(index=False).strip() in journals:\n",
    "        return journals[journal.to_string(index=False).strip()]\n",
    "    else:\n",
    "        return 0\n",
    "        #print(journals)\n",
    "        #print(journal.to_string(index=False).strip())\n",
    "        #print()\n",
    "        #sleep(5)\n",
    "\n",
    "def prepJournals(fname):\n",
    "    f = open(fname, 'r')\n",
    "    journals = dict()\n",
    "    for line in f.readlines():\n",
    "        if line[-1] == '\\n':\n",
    "            line = line[:-1]\n",
    "        ls = line.split(\" \")\n",
    "        journals[ls[1]] = ls[0]\n",
    "    f.close()\n",
    "\n",
    "    return journals\n",
    "    \n",
    "def readAnserini(fname):\n",
    "    res=[]\n",
    "    f = open(fname, 'r')\n",
    "    #f.readline()\n",
    "    for line in f.readlines():\n",
    "        vals = line.strip().split(\" \")\n",
    "        #topic, rank, cord_id, score\n",
    "        res.append([vals[0], vals[3], vals[2], vals[4]])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py:2822: DtypeWarning: Columns (13,14) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded doc2vec model /home/tmschoegje/Desktop/caos-19/covid-doc2vec.model\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (13,14) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-559521e0ad6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadAnserini\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mjournals\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mprepJournals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/tmschoegje/Desktop/caos-19/round3/journalpriors.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mresults_reranked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrerank2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadTopics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/tmschoegje/Desktop/caos-19/trecdata/topics-rnd3.xml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjournals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0mwriteBM25results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_reranked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/home/tmschoegje/Desktop/caos-19/ruir-run2-\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndcg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/tmschoegje/Desktop/caos-19/ruir-run2-\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqrelname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-559521e0ad6d>\u001b[0m in \u001b[0;36mrerank2\u001b[0;34m(results, model, topics, mixer, journals)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mjscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetJPrior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjournals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-75be5c00b74e>\u001b[0m in \u001b[0;36mgetJPrior\u001b[0;34m(cord_uid, metadata, journals)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetJPrior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcord_uid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjournals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0mjournal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cord_uid'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcord_uid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'journal'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;31m#print(journals)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;31m#print(journal.to_string())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tmschoegje/.local/lib/python3.6/site-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tmschoegje/.local/lib/python3.6/site-packages/pandas/core/ops/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0mrvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomparison_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tmschoegje/.local/lib/python3.6/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomp_method_OBJECT_ARRAY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tmschoegje/.local/lib/python3.6/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#adapted from round1code.py\n",
    "#here we try to rerank the results of\n",
    "#'/home/tmschoegje/Desktop/caos-19/trecdata/testtasktext'\n",
    "#using the doc2vec approach, and tune it using ndcg*\n",
    "\n",
    "def rerank(results, model, topics, mixer=1):\n",
    "    #print(\"reranking\")\n",
    "    metadata = prepTREC('/home/tmschoegje/Desktop/caos-19/trecdata/docids-rnd3.txt')\n",
    "    \n",
    "    dists = []\n",
    "    scores = []\n",
    "    #note that this result is formatted slightly differently (see readanserini).. legacy\n",
    "    for result in results:\n",
    "        sha = topics[int(result[0]) - 1]\n",
    "#        print(sha)\n",
    "        taskvector = taskvectors[sha[1]]#sha[0])\n",
    "#        print(result[2])\n",
    "        dist = np.linalg.norm(taskvector-get_doc_vector(model, getAbstract(result[2], metadata)))\n",
    "#        print(dist)\n",
    "        dists.append(dist)\n",
    "        scores.append(float(result[3]))\n",
    "#        print(result)\n",
    "#    print(np.array(dists).shape)\n",
    "#    print(dists[0:20])\n",
    "#    print(scores[0:20])\n",
    "    #print(results[0:5])\n",
    "    #print(len(results))\n",
    "    \n",
    "    #normalize distances\n",
    "    distsnorm = [float(i)/max(dists) for i in dists]\n",
    "    scoresnorm = [float(i)/max(scores) for i in scores]\n",
    "    \n",
    "    newscores = []\n",
    "    for i, val in enumerate(distsnorm):\n",
    "        newscores.append(mixer * distsnorm[i] + scoresnorm[i])\n",
    "        results[i][3] = mixer * distsnorm[i] + scoresnorm[i]\n",
    "    \n",
    "    #Some ugly/quick sorting\n",
    "    def sort_key0(item):\n",
    "        return item[3]\n",
    "    def sort_key1(item):\n",
    "        return item[0]\n",
    "\n",
    "    results = sorted(results, key=sort_key0, reverse=True)\n",
    "    results = sorted(results, key=sort_key1, reverse=False)\n",
    "    \n",
    "    #print(results[0:5])\n",
    "    #return neworder\n",
    "    return results\n",
    "\n",
    "\n",
    "def rerank2(results, model, topics, mixer, journals):\n",
    "    metadata = prepTREC('/home/tmschoegje/Desktop/caos-19/trecdata/docids-rnd3.txt')\n",
    "    \n",
    "    jscores = []\n",
    "    scores = []\n",
    "    for result in results:\n",
    "        jscores.append(getJPrior(result[2], metadata, journals))\n",
    "        scores.append(float(result[3]))\n",
    "    \n",
    "    for i, val in enumerate(jscores):\n",
    "        results[i][3] = mixer * float(jscores[i]) + scores[i]\n",
    "    \n",
    "    #Some ugly/quick sorting\n",
    "    def sort_key0(item):\n",
    "        return item[3]\n",
    "    def sort_key1(item):\n",
    "        return item[0]\n",
    "\n",
    "    results = sorted(results, key=sort_key0, reverse=True)\n",
    "    results = sorted(results, key=sort_key1, reverse=False)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "#prepare results in submission format\n",
    "def writeBM25results(results, runtitle):\n",
    "    f = open(runtitle, \"w\")\n",
    "    #topic, rank, cord_id, score\n",
    "    for result in results:\n",
    "        #print(result)\n",
    "        f.write(result[0] + \" Q0 \" + result[2] + \" 1 \" + str(result[3]) + \" \" + runtitle + \"\\n\")\n",
    "    f.close()\n",
    "\n",
    "    \n",
    "#Now we train or load a doc2vec model\n",
    "model = preparedoc2vec(\"/home/tmschoegje/Desktop/caos-19/covid-doc2vec.model\", prepTREC('/home/tmschoegje/Desktop/caos-19/trecdata/docids-rnd1.txt'))\n",
    "qrelname = qrel_path\n",
    "\n",
    "for m3 in np.linspace(0, 30, 10):\n",
    "    print(m3)\n",
    "    runname = '/home/tmschoegje/Desktop/caos-19/trecdata/qexp-q0.4n0.4t0.2.run'#/home/tmschoegje/Desktop/caos-19/runs/testrun-best-rnd4.run'\n",
    "\n",
    "    results = readAnserini(runname)\n",
    "    journals= prepJournals('/home/tmschoegje/Desktop/caos-19/round3/journalpriors.txt')\n",
    "    results_reranked = rerank2(results, model, readTopics(\"/home/tmschoegje/Desktop/caos-19/trecdata/topics-rnd3.xml\"), m3, journals)\n",
    "    writeBM25results(results_reranked, \"/home/tmschoegje/Desktop/caos-19/ruir-run2-\" + str(m3))\n",
    "    print(ndcg(\"/home/tmschoegje/Desktop/caos-19/ruir-run2-\" + str(m3), qrelname))\n",
    "print('ready')\n",
    "\"\"\"\n",
    "\n",
    "#create a list of the taskvectors\n",
    "taskvectors = []\n",
    "for task in list_of_tasks:\n",
    "    taskvectors.append(get_doc_vector(model, task))\n",
    "\n",
    "\n",
    "results_reranked = rerank(results, model, readTopics(\"/home/tmschoegje/Desktop/caos-19/trecdata/topics-rnd2.xml\"), mixer=100)\n",
    "writeBM25results(results_reranked, \"/home/tmschoegje/Desktop/caos-19/doc2vectest3\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = readAnserini('/home/tmschoegje/Desktop/caos-19/trecdata/query0.2narr0.4task0.4.run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (13,14) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'taskvectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2359cdfa1753>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mrunname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/tmschoegje/Desktop/caos-19/mixertest\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mresults_reranked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrerank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadTopics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/tmschoegje/Desktop/caos-19/trecdata/topics-rnd2.xml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmixer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mwriteBM25results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_reranked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'results for m '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndcg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqrelname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-559521e0ad6d>\u001b[0m in \u001b[0;36mrerank\u001b[0;34m(results, model, topics, mixer)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0msha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#        print(sha)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mtaskvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtaskvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#sha[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m#        print(result[2])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaskvector\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mget_doc_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetAbstract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'taskvectors' is not defined"
     ]
    }
   ],
   "source": [
    "qrelname = qrel_path#'/home/tmschoegje/Desktop/caos-19/trecdata/qrels-rnd2.txt'\n",
    "results = readAnserini('/home/tmschoegje/Desktop/caos-19/trecdata/query0.2narr0.4task0.4.run')\n",
    "for m in np.linspace(0, 1, 10):\n",
    "    runname = \"/home/tmschoegje/Desktop/caos-19/mixertest\" + str(m)\n",
    "    results_reranked = rerank(results, model, readTopics(\"/home/tmschoegje/Desktop/caos-19/trecdata/topics-rnd2.xml\"), mixer=m)\n",
    "    writeBM25results(results_reranked, runname)\n",
    "    print('results for m ' + str(m) + ': ' + str(ndcg(runname, qrelname)))\n",
    "\n",
    "for m in np.linspace(1, 11, 10):\n",
    "    runname = \"/home/tmschoegje/Desktop/caos-19/mixertest\" + str(m)\n",
    "    results_reranked = rerank(results, model, readTopics(\"/home/tmschoegje/Desktop/caos-19/trecdata/topics-rnd2.xml\"), mixer=m)\n",
    "    writeBM25results(results_reranked, runname)\n",
    "    print('results for m ' + str(m) + ': ' + str(ndcg(runname, qrelname)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#print(ndcg(runname, qrelname))\n",
    "#print(ndcg('/home/tmschoegje/Desktop/caos-19/trecdata/testtasktext3', qrelname))\n",
    "\n",
    "#anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_split = .60\n",
    "nar_split = .25\n",
    "task_split = .15\n",
    "\n",
    "#should we filter old qrels etc for round 3?  i.e. False = generate using valid doc ids etc\n",
    "unfiltered = True\n",
    "\n",
    "def testrun2(fname, pq, pnarr, ptask):\n",
    "\n",
    "    qrel_seen = 0\n",
    "    total = 0\n",
    "\n",
    "    with open(fname, 'w') as f:\n",
    "        for topicno, topic in tqdm(enumerate(topics)):\n",
    "                task = tasks[rnd3classes[topicno]]\n",
    "                topic = topic.split(' ')\n",
    "                nar = nars[topicno]\n",
    "                nar = nar.split(' ')    \n",
    "                new_task = []\n",
    "                for t in task:\n",
    "                    for nt in t.split(' '):\n",
    "                        new_task.append(nt)\n",
    "                #use with kaggle task text\n",
    "                task = new_task\n",
    "                #task_confidence = 1\n",
    "                \n",
    "                #use with qrel task text\n",
    "                #task = tasks_weights[topic_task_no[topicno]]\n",
    "                task_confidence = rnd3confidence[rnd3classes[topicno]]\n",
    "                \n",
    "                query = build_query(topic, nar, task, pq, pnarr, ptask, task_confidence, rnd3classes[topicno])\n",
    "                \n",
    "                #for ranking qrels\n",
    "                #mast\n",
    "                \n",
    "                #topic, cord, rel, round\n",
    "#                qrels = getqrels(qrelname)\n",
    "#                hits = []\n",
    "                \n",
    "#                print(query)\n",
    "#                for qrel in qrels:\n",
    "#                    score = index_utils.compute_query_document_score(qrel[1], query)\n",
    "#                    hits.append([qrel].extend(score))\n",
    "#                    print(hits)\n",
    "                    \n",
    "#                rankedhits = sorted(hits, key=lambda x: x[4])\n",
    "                \n",
    "#                print(hits[0:5])\n",
    "#                print(rankedhits[0:5])\n",
    "#                print()\n",
    "                \n",
    "                \n",
    "                hits = searcher.search(query, 20000) #5000 for filtering\n",
    "\n",
    "                i = 0\n",
    "                j = 0\n",
    "                seen = set()\n",
    "                \n",
    "\n",
    "                #for regular ranking\n",
    "                while j < len(hits):#i < 100:   #for generating the 100 end results\n",
    "                    hit = hits[j]\n",
    "                    # print(hit)\n",
    "                    if(unfiltered):\n",
    "                        #pass\n",
    "                        if(hit.docid.split('.')[0] not in judged[topicno+1]) or hit.docid.split('.')[0] in seen:\n",
    "                            j+=1\n",
    "                            continue\n",
    "                    else:\n",
    "                        if hit.docid.split('.')[0] in seen or hit.docid.split('.')[0] in judged[topicno+1] or hit.docid.split('.')[0] not in valid:\n",
    "                            j+=1\n",
    "                            continue\n",
    "                    f.write(f'{topicno+1} Q0 {hit.docid.split(\".\")[0]} {i+1} {hit.score} {fname[42:-4]}\\n')\n",
    "                    i+=1\n",
    "                    j+=1\n",
    "                    seen |= {hit.docid.split('.')[0]}\n",
    "\n",
    "                    if hit.docid.split('.')[0] in judged[topicno+1]:\n",
    "                        qrel_seen +=1\n",
    "                    total+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4719bbb256e44aca44f7356ac1635ad"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coronavirus origin\n",
      "coronavirus response to weather changes\n",
      "coronavirus immunity\n",
      "how do people die from the coronavirus\n",
      "animal models of COVID-19\n",
      "coronavirus test rapid testing\n",
      "serological tests for coronavirus\n",
      "coronavirus under reporting\n",
      "coronavirus in Canada\n",
      "coronavirus social distancing impact\n",
      "coronavirus hospital rationing\n",
      "coronavirus quarantine\n",
      "how does coronavirus spread\n",
      "coronavirus super spreaders\n",
      "coronavirus outside body\n",
      "how long does coronavirus survive on surfaces\n",
      "coronavirus clinical trials\n",
      "masks prevent coronavirus\n",
      "what alcohol sanitizer kills coronavirus\n",
      "coronavirus and ACE inhibitors\n",
      "coronavirus mortality\n",
      "coronavirus heart impacts\n",
      "coronavirus hypertension\n",
      "coronavirus diabetes\n",
      "coronavirus biomarkers\n",
      "coronavirus early symptoms\n",
      "coronavirus asymptomatic\n",
      "coronavirus hydroxychloroquine\n",
      "coronavirus drug repurposing\n",
      "coronavirus remdesivir\n",
      "difference between coronavirus and flu\n",
      "coronavirus subtypes\n",
      "coronavirus vaccine candidates\n",
      "coronavirus recovery\n",
      "coronavirus public datasets\n",
      "SARS-CoV-2 spike structure\n",
      "SARS-CoV-2 phylogenetic analysis\n",
      "COVID inflammatory response\n",
      "COVID-19 cytokine storm\n",
      "coronavirus mutations\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#generate baseline\n",
    "\n",
    "\n",
    "bm25_split = .60\n",
    "nar_split = .25\n",
    "task_split = .15\n",
    "\n",
    "#should we filter old qrels etc for round 3?  i.e. False = generate using valid doc ids etc\n",
    "unfiltered = False\n",
    "\n",
    "def testrun2(fname, pq=1, pnarr=0, ptask=0):\n",
    "\n",
    "    qrel_seen = 0\n",
    "    total = 0\n",
    "\n",
    "    with open(fname, 'w') as f:\n",
    "        for topicno, topic in tqdm(enumerate(topics)):\n",
    "                task = tasks[rnd3classes[topicno]]\n",
    "                topic = topic.split(' ')\n",
    "                nar = nars[topicno]\n",
    "                nar = nar.split(' ')    \n",
    "                new_task = []\n",
    "                for t in task:\n",
    "                    for nt in t.split(' '):\n",
    "                        new_task.append(nt)\n",
    "                #use with kaggle task text\n",
    "                task = new_task\n",
    "                #task_confidence = 1\n",
    "                \n",
    "                #use with qrel task text\n",
    "                #task = tasks_weights[topic_task_no[topicno]]\n",
    "                task_confidence = rnd3confidence[rnd3classes[topicno]]\n",
    "                \n",
    "                #query = build_query(topic, nar, task, pq, pnarr, ptask, task_confidence, rnd3classes[topicno])\n",
    "                \n",
    "                query = ' '.join(topic)\n",
    "                print(query)\n",
    "                \n",
    "                hits = searcher.search(query, 5000) #5000 for filtering\n",
    "\n",
    "                i = 0\n",
    "                j = 0\n",
    "                seen = set()\n",
    "                \n",
    "\n",
    "                #for regular ranking\n",
    "                while i < 100:   #for generating the 100 end results\n",
    "                    hit = hits[j]\n",
    "                    # print(hit)\n",
    "                    if(unfiltered):\n",
    "                        #pass\n",
    "                        if(hit.docid.split('.')[0] not in judged[topicno+1]) or hit.docid.split('.')[0] in seen:\n",
    "                            j+=1\n",
    "                            continue\n",
    "                    else:\n",
    "                        if hit.docid.split('.')[0] in seen or hit.docid.split('.')[0] in judged[topicno+1] or hit.docid.split('.')[0] not in valid:\n",
    "                            j+=1\n",
    "                            continue\n",
    "                    f.write(f'{topicno+1} Q0 {hit.docid.split(\".\")[0]} {i+1} {hit.score} {fname[42:-4]}\\n')\n",
    "                    i+=1\n",
    "                    j+=1\n",
    "                    seen |= {hit.docid.split('.')[0]}\n",
    "\n",
    "                    if hit.docid.split('.')[0] in judged[topicno+1]:\n",
    "                        qrel_seen +=1\n",
    "                    total+=1\n",
    "                    \n",
    "testrun2('./baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrel_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ru-tn-exp-rnd2.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ru-t-exp-rnd2.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Practical recommendations for the management of diabetes in patients with COVID-19\n",
      "Diabetes is one of the most important comorbidities linked to the severity of all three known human pathogenic coronavirus infections, including severe acute respiratory syndrome coronavirus 2. Patients with diabetes have an increased risk of severe complications including Adult Respiratory Distress Syndrome and multi-organ failure. Depending on the global region, 20–50% of patients in the coronavirus disease 2019 (COVID-19) pandemic had diabetes. Given the importance of the link between COVID-19 and diabetes, we have formed an international panel of experts in the field of diabetes and endocrinology to provide some guidance and practical recommendations for the management of diabetes during the pandemic. We aim to briefly provide insight into potential mechanistic links between the novel coronavirus infection and diabetes, present practical management recommendations, and elaborate on the differential needs of several patient groups.\n",
      "From January, 2020, we have been facing an unprecedented outbreak of coronavirus disease 2019 (COVID-19) caused by a novel coronavirus, severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), which has now become a global catastrophe. Data from the early months of 2020 suggest that most people with COVID-19 have comorbidities, the most prevalent of which are diabetes, cardiovascular disease, and hypertension.1 A significant association with worse outcomes is seen in people with these comorbidities.1 Studies have also shown that COVID-19 is associated with hyperglycaemia particularly in the elderly with type 2 diabetes.2 In view of many uncertainties with COVID-19, a faculty of representatives from primary and specialist care have developed a consensus document on the management of diabetes for people at risk of or with confirmed COVID-19 for use in both primary and specialist care. The brief practical recommendations authored by this group were convened virtually. The recommendations are based on queries that have been emphasised to be important by clinicians, questions that have been raised by colleagues and social media, and recommendations guided by using focused-literature review. Clinical decision making in the management of diabetes is already complex and in normal circumstances we recommend clinicians follow guidelines for management of people with diabetes. However, the recommendations authored by our group add to the existing guidelines by considering specific points for the management of patients with diabetes and COVID-19 disease or at risk for metabolic disease.\n",
      "Practical recommendations for the management of diabetes in patients with COVID-19\n",
      "Diabetes is one of the most important comorbidities linked to the severity of all three known human pathogenic coronavirus infections, including severe acute respiratory syndrome coronavirus 2. Patients with diabetes have an increased risk of severe complications including Adult Respiratory Distress Syndrome and multi-organ failure. Depending on the global region, 20–50% of patients in the coronavirus disease 2019 (COVID-19) pandemic had diabetes. Given the importance of the link between COVID-19 and diabetes, we have formed an international panel of experts in the field of diabetes and endocrinology to provide some guidance and practical recommendations for the management of diabetes during the pandemic. We aim to briefly provide insight into potential mechanistic links between the novel coronavirus infection and diabetes, present practical management recommendations, and elaborate on the differential needs of several patient groups.\n",
      "All patients without diabetes and particularly when at high risk for metabolic disease who have contracted the viral infection need to be monitored for new onset diabetes that might be triggered by the virus. All patients with COVID-19 disease and diabetes require continuous and reliable glycaemic control as suggested in the flowchart.\n",
      "Practical recommendations for the management of diabetes in patients with COVID-19\n",
      "Diabetes is one of the most important comorbidities linked to the severity of all three known human pathogenic coronavirus infections, including severe acute respiratory syndrome coronavirus 2. Patients with diabetes have an increased risk of severe complications including Adult Respiratory Distress Syndrome and multi-organ failure. Depending on the global region, 20–50% of patients in the coronavirus disease 2019 (COVID-19) pandemic had diabetes. Given the importance of the link between COVID-19 and diabetes, we have formed an international panel of experts in the field of diabetes and endocrinology to provide some guidance and practical recommendations for the management of diabetes during the pandemic. We aim to briefly provide insight into potential mechanistic links between the novel coronavirus infection and diabetes, present practical management recommendations, and elaborate on the differential needs of several patient groups.\n",
      "Diabetes is a primary risk factor for the development of severe pneumonia and a septic course due to virus infections and occurs in around 20% of patients.3, 4 Diabetes was identified as a major contributor to disease severity and mortality in Middle East Respiratory Syndrome (MERS-CoV).5 Evidence from epidemiological observations in regions heavily affected by SARS-CoV-2 and reports from the Centers for Disease Control and Prevention (CDC) and other national health centres and hospitals showed that the risk of a fatal outcome from COVID-19 is up to 50% higher in patients with diabetes than in those who do not have diabetes.6 There are several hypotheses to explain the increased incidence and severity of COVID-19 infection in people with diabetes. In general, people with all forms of diabetes are at increased risk of infection because of defects in innate immunity affecting phagocytosis, neutrophil chemotaxis, and cell-mediated immunity; however, the high frequency of diabetes in serious cases of COVID-19 could potentially reflect the higher prevalence of type 2 diabetes in older people. Furthermore, diabetes in older age is associated with cardiovascular disease, which in itself could help to explain the association with fatal outcomes of COVID-19.\n",
      "Practical recommendations for the management of diabetes in patients with COVID-19\n",
      "Diabetes is one of the most important comorbidities linked to the severity of all three known human pathogenic coronavirus infections, including severe acute respiratory syndrome coronavirus 2. Patients with diabetes have an increased risk of severe complications including Adult Respiratory Distress Syndrome and multi-organ failure. Depending on the global region, 20–50% of patients in the coronavirus disease 2019 (COVID-19) pandemic had diabetes. Given the importance of the link between COVID-19 and diabetes, we have formed an international panel of experts in the field of diabetes and endocrinology to provide some guidance and practical recommendations for the management of diabetes during the pandemic. We aim to briefly provide insight into potential mechanistic links between the novel coronavirus infection and diabetes, present practical management recommendations, and elaborate on the differential needs of several patient groups.\n",
      "A second potential mechanism that might explain the link between COVID-19 and diabetes involves the dipeptidyl peptidase-4 (DPP-4) enzyme, which is commonly targeted pharmacologically in people with type 2 diabetes. In cell studies, DPP-4 was identified as a functional receptor for human coronavirus-Erasmus Medical Center (hCoV-EMC), the virus responsible for MERS.11 Antibodies directed against DPP-4 inhibited hCoV-EMC infection of primary cells. DPP-4 enzyme is an ubiquitously expressed type II transmembrane glycoprotein. It plays a major role in glucose and insulin metabolism but also increases inflammation in type 2 diabetes.12 Whether these mechanisms also apply to COVID-19 and whether diabetes treatment with DPP-4 inhibitors in clinical practice influences the course of the infection is currently unknown, but, if these mechanisms translate to SARS-CoV-2, the use of these agents could reduce DPP-4 concentrations and could provide therapeutic opportunities for treatment of COVID-19.12\n",
      "\n",
      "Coronavirus infections and type 2 diabetes-shared pathways with therapeutic implications\n",
      "Individuals with diabetes are at increased risk for bacterial, mycotic, parasitic and viral infections. The severe acute respiratory syndrome (SARS)-CoV2 (also referred to as COVID-19) coronavirus pandemic highlights the importance of understanding shared disease pathophysiology potentially informing therapeutic choices in individuals with Type 2 diabetes (T2D). Two coronavirus receptor proteins, Angiotensin Converting Enzyme 2 (ACE2) and Dipeptidyl Peptidase-4 (DPP4) are also established transducers of metabolic signals and pathways regulating inflammation, renal and cardiovascular physiology, and glucose homeostasis. Moreover, glucose-lowering agents such as the DPP4 inhibitors, widely used in subjects with T2D, are known to modify the biological activities of multiple immunomodulatory substrates. Here we review the basic and clinical science spanning the intersections of diabetes, coronavirus infections, ACE2, and DPP4 biology, highlighting clinical relevance and evolving areas of uncertainty underlying the pathophysiology and treatment of T2D in the context of coronavirus infection\n",
      "The rapid flow of new clinical information stemming from the SARS-CoV-2 epidemic requires ongoing scrutiny to understand the prudent use, risks and benefits of individual glucose-lowering agents and related medications commonly used in subjects with diabetes at risk of, or hospitalized with coronavirus-related infections. Moreover, the current pandemic highlights the importance of opportunities for continuing and expanding innovative delivery of diabetes care, through use of wearable and portable monitoring devices, and regular communication between people with diabetes, and their health care providers. A c c e p t e d M a n u s c r i p t 26  ACE2 and DPP4 are coronavirus receptors  ACE2 and DPP4 control inflammation, and cardiometabolic physiology  DPP4 is a MERS-CoV but not a SARS-Cov-2 receptor  DPP4 inhibitors do not meaningfully modify immune response in human subjects  SARS-CoV-2 hospitalizations are more common in people with diabetes and obesity  Acute SARS-Co-V-2 illness requires re-evaluation of medications used for type 2 diabetes  Insulin is the glucose-lowering therapy of choice for acute coronavirus-related illness in hospital ACE2 ACE2\n"
     ]
    }
   ],
   "source": [
    "hits = searcher.search('coronavirus diabetes', 5)\n",
    "for hit in hits:\n",
    "    print(hit.contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
